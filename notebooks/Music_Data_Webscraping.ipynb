{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Music Data from University of Waterloo\n",
        "In this lab, we will extract the music dataset provided by [Dave Tompkins](https://cs.uwaterloo.ca/~dtompkin/music/)\n",
        "Associate Professor at University of Waterloo. To extract the data, we utilized the BeautifulSoup library to scrape music data spanning ten consecutive years (1997–2006). This dataset provided song titles and artist names, which we later used to filter music-related search queries from the AOL search logs."
      ],
      "metadata": {
        "id": "W8iuigJlily4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the beautifulsoup Library\n",
        "We import beautifulsoup library to extract, navigate, and manipulate HTML/XML content efficiently."
      ],
      "metadata": {
        "id": "gIF-D3AtiMgk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyyccP0V-DZA",
        "outputId": "d9b156c0-1583-4d1a-8111-9dbe86e14f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting beautifulsoup\n",
            "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "Ux4XhVvkDtC4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping Music Data (1997–2006) Using BeautifulSoup\n",
        "In the code below we iterate through each year, construct the corresponding URL, and send a request to retrieve the webpage content. If the request is successful, it parses the HTML to locate a table containing music information. The script extracts key details such as artist, song title, duration, BPM, year, and genre, ensuring each row has the expected structure before storing the data in a list. Finally, it prints the first 10 retrieved songs, or a message if no data was found."
      ],
      "metadata": {
        "id": "KiSa4yEoiksq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GUsW-UlM-fkZ"
      },
      "outputs": [],
      "source": [
        "years = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006]\n",
        "all_songs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SknP3PXJ1Mhi",
        "outputId": "fa69e5f6-3ab9-4306-d40c-2c8db70eb0fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "Headers found: ['', 'ARTIST', 'TITLE', 'TIME', 'BPM', 'YEAR', 'GENRE', 'DISC-TRACK', 'DETAILS']\n",
            "Data has been written to 'songs_data.csv'.\n",
            "{'artist': 'Spice Girls', 'song': '2 Become 1', 'time': '3:57', 'bpm': '144.0', 'year': '1997', 'details': '../track/HOTTRACK_046/HOTTRACK_046-10.html', 'genre': 'Rock/Pop', 'album': 'Hott Tracks 046', 'track num': '10'}\n",
            "{'artist': 'Spice Girls', 'song': '2 Become 1', 'time': '3:58', 'bpm': '144.0', 'year': '1997', 'details': '../track/POBO1997_001/POBO1997_001-12.html', 'genre': 'Rock/Pop', 'album': 'Promo Only - Best of 1997 - Disc 1', 'track num': '12'}\n",
            "{'artist': 'Hooverphonic', 'song': '2 Wicky', 'time': '4:08', 'bpm': '102.2', 'year': '1997', 'details': '../track/HOTTRACK_051/HOTTRACK_051-17.html', 'genre': 'Alternative', 'album': 'Hott Tracks 051', 'track num': '17'}\n",
            "{'artist': 'Zuckerbaby', 'song': '2wice As Hard', 'time': '3:50', 'bpm': '106.8', 'year': '1997', 'details': '../track/ZUKRBABY_STD/ZUKRBABY_STD-04.html', 'genre': 'Rock', 'album': 'Zuckerbaby', 'track num': '4'}\n",
            "{'artist': 'Alana Davis', 'song': '32 Flavors', 'time': '3:44', 'bpm': '94.3', 'year': '1997', 'details': '../track/ALNADAVS_BIM/ALNADAVS_BIM-01.html', 'genre': 'Rock/Pop', 'album': 'Blame It on Me', 'track num': '1'}\n",
            "{'artist': 'Alana Davis', 'song': '32 Flavors', 'time': '3:43', 'bpm': '94.3', 'year': '1997', 'details': '../track/HOTTRACK_057/HOTTRACK_057-15.html', 'genre': 'Rock/Pop', 'album': 'Hott Tracks 057', 'track num': '15'}\n",
            "{'artist': 'Alana Davis', 'song': '32 Flavors', 'time': '3:44', 'bpm': '94.3', 'year': '1997', 'details': '../track/POBO1997_002/POBO1997_002-17.html', 'genre': 'Rock/Pop', 'album': 'Promo Only - Best of 1997 - Disc 2', 'track num': '17'}\n",
            "{'artist': 'Alana Davis', 'song': '32 Flavors', 'time': '3:46', 'bpm': '94.3', 'year': '1997', 'details': '../track/WOMNSONG_001/WOMNSONG_001-06.html', 'genre': 'Rock/Pop', 'album': 'Women & Songs', 'track num': '6'}\n",
            "{'artist': 'Matchbox Twenty', 'song': '3Am', 'time': '3:44', 'bpm': '108.0', 'year': '1997', 'details': '../track/HOTTROCK_022/HOTTROCK_022-04.html', 'genre': 'Rock', 'album': 'Hott Rocks 022', 'track num': '4'}\n",
            "{'artist': 'Aaliyah', 'song': \"4 Page Leter [Timbaland's Main Mix]\", 'time': '4:32', 'bpm': '119.0', 'year': '1997', 'details': '../track/HOTTRACK_042/HOTTRACK_042-16.html', 'genre': 'R&B', 'album': 'Hott Tracks 042', 'track num': '16'}\n"
          ]
        }
      ],
      "source": [
        "for year in years:\n",
        "    url = f'https://cs.uwaterloo.ca/~dtompkin/music/year/{year}.html'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the page is accessible\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve data for {year}. Status code: {response.status_code}\")\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the table with the class 'music'\n",
        "    table = soup.find('table', class_='music')\n",
        "\n",
        "    if not table:\n",
        "        print(f\"No table found for {year}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    rows = table.find_all('tr')\n",
        "\n",
        "    if not rows:\n",
        "        print(f\"No rows found for {year}. Skipping...\")\n",
        "        continue\n",
        "\n",
        "    # Find the index of the header row to determine the structure\n",
        "    header = rows[0]\n",
        "    headers = [th.get_text(strip=True) for th in header.find_all('td')]\n",
        "    print(f\"Headers found: {headers}\")  # For debugging\n",
        "    counter = 0\n",
        "    # Extract data from each row\n",
        "    for row in rows[1:]:  # Skipping the header row\n",
        "        cols = row.find_all('td')\n",
        "\n",
        "        # Ensure the row contains the expected number of columns\n",
        "        if len(cols) >= 8:  # Expecting at least 8 columns based on the header structure\n",
        "            song_info = {\n",
        "                'artist': cols[1].get_text(strip=True),  # ARTIST\n",
        "                'song': cols[2].get_text(strip=True),   # TITLE\n",
        "                'time': cols[3].get_text(strip=True),   # TIME\n",
        "                'bpm': cols[4].get_text(strip=True),     # BPM\n",
        "                'year': cols[5].get_text(strip=True),    # YEAR\n",
        "                'details': cols[8].find('a')['href']  # Link inside the details column\n",
        "            }\n",
        "\n",
        "            # Get the link from the details column and scrape the third table\n",
        "            detail_url = song_info['details']\n",
        "            counter += 1\n",
        "\n",
        "\n",
        "            if not detail_url:\n",
        "                song_info['genre'] = \"***\"\n",
        "            detail_url = detail_url.replace(\"../\", \"\")\n",
        "\n",
        "            detail_url = f'https://cs.uwaterloo.ca/~dtompkin/music/{detail_url}'\n",
        "\n",
        "            # If the URL is relative, prepend the base URL\n",
        "            '''if detail_url.startswith('/'):\n",
        "                detail_url = detail_url.replace(\"../\", \"\")\n",
        "                detail_url = f'https://cs.uwaterloo.ca/~dtompkin/music/{detail_url}'\n",
        "                print('detail_url', detail_url)'''\n",
        "\n",
        "            # Request the detail page\n",
        "            detail_response = requests.get(detail_url)\n",
        "            if detail_response.status_code == 200:\n",
        "                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
        "\n",
        "\n",
        "                # Find all tables with the class 'simpler'\n",
        "                simpler_tables = detail_soup.find_all('table', class_='simple')\n",
        "\n",
        "                if len(simpler_tables) >= 3:\n",
        "                    second_table = simpler_tables[1]\n",
        "                    third_table = simpler_tables[2]  # Get the third table\n",
        "                    rows_in_table_album = second_table.find_all('tr')\n",
        "                    rows_in_table_genre = third_table.find_all('tr')\n",
        "\n",
        "                    selected_data_album_name = [cell.get_text(strip=True) for cell in rows_in_table_album[1].find_all('td')]\n",
        "                    selected_data_album_track = [cell.get_text(strip=True) for cell in rows_in_table_album[2].find_all('td')]\n",
        "\n",
        "\n",
        "                    # Take the second row if there are two, otherwise the first row\n",
        "                    if len(rows_in_table_genre) >= 3:\n",
        "                        selected_row = rows_in_table_genre[2]  # Second row\n",
        "                    else:\n",
        "                        selected_row = rows_in_table_genre[1]  # First row\n",
        "\n",
        "                    # Extract the text or information you need from the selected row\n",
        "                    selected_data_genre = [cell.get_text(strip=True) for cell in selected_row.find_all('td')]\n",
        "\n",
        "\n",
        "                    # Optionally, add this information to the song_info dictionary\n",
        "                    song_info['genre'] = selected_data_genre[1]\n",
        "                    song_info['album'] = selected_data_album_name[1]\n",
        "                    song_info['track num'] = selected_data_album_track[1]\n",
        "\n",
        "                else:\n",
        "                    print(f\"Could not find the third table for {song_info['song']}.\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed to retrieve details page for {song_info['song']}. Status code: {detail_response.status_code}\", '\\n', detail_url, '\\n', counter)\n",
        "\n",
        "            # Append the song info to the list\n",
        "            all_songs.append(song_info)\n",
        "\n",
        "    with open(f'MusicMetadata_{year}_.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "      fieldnames = ['artist', 'song', 'time', 'bpm', 'year', 'genre', 'details', 'album', 'track num']  # Column names for the CSV\n",
        "      writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writeheader()  # Write the header row\n",
        "      writer.writerows(all_songs)  # Write the song data\n",
        "\n",
        "    print(f\"Data has been written to 'songs_data.csv'.\")\n",
        "\n",
        "# Display the first 10 songs from all years\n",
        "if all_songs:\n",
        "    for song in all_songs[:10]:\n",
        "        print(song)\n",
        "else:\n",
        "    print(\"No songs retrieved.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}